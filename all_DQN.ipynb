{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Implementation of Vanilla, Double and Clipped Double DQN"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-14T02:03:28.600493Z","iopub.status.busy":"2024-01-14T02:03:28.599661Z","iopub.status.idle":"2024-01-14T02:03:52.305662Z","shell.execute_reply":"2024-01-14T02:03:52.304616Z","shell.execute_reply.started":"2024-01-14T02:03:28.600462Z"},"trusted":true},"outputs":[],"source":["#!pip install gymnasium[atari]\n","#!pip install gymnasium[accept-rom-license]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-14T02:03:52.308561Z","iopub.status.busy":"2024-01-14T02:03:52.308234Z","iopub.status.idle":"2024-01-14T02:04:04.283141Z","shell.execute_reply":"2024-01-14T02:04:04.282092Z","shell.execute_reply.started":"2024-01-14T02:03:52.308530Z"},"trusted":true},"outputs":[],"source":["#!pip install \"gym[accept-rom-license, atari]\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-14T02:04:04.285198Z","iopub.status.busy":"2024-01-14T02:04:04.284876Z","iopub.status.idle":"2024-01-14T02:04:04.296180Z","shell.execute_reply":"2024-01-14T02:04:04.295283Z","shell.execute_reply.started":"2024-01-14T02:04:04.285168Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from keras.models import Model\n","from keras.layers import Conv2D, Flatten, Dense\n","from stable_baselines3.common.vec_env.vec_frame_stack import VecFrameStack\n","from stable_baselines3.common.env_util import make_atari_env\n","import random\n","import os, psutil \n","from tqdm import tqdm\n","from collections import deque\n","import json\n","import time\n","import gc"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-14T02:04:04.298736Z","iopub.status.busy":"2024-01-14T02:04:04.298434Z","iopub.status.idle":"2024-01-14T02:04:04.312824Z","shell.execute_reply":"2024-01-14T02:04:04.311986Z","shell.execute_reply.started":"2024-01-14T02:04:04.298702Z"},"trusted":true},"outputs":[],"source":["class LinearIterator:\n","    def __init__(self, start, end, steps):\n","        self.start = start\n","        self.end = end\n","        self.steps = steps\n","    \n","    def value(self, step):\n","        return max(self.start + (self.end - self.start) * step / self.steps, self.end)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-14T02:04:04.314424Z","iopub.status.busy":"2024-01-14T02:04:04.314150Z","iopub.status.idle":"2024-01-14T02:04:04.330543Z","shell.execute_reply":"2024-01-14T02:04:04.329810Z","shell.execute_reply.started":"2024-01-14T02:04:04.314401Z"},"trusted":true},"outputs":[],"source":["def cpu_stats():\n","    pid = os.getpid()\n","    py = psutil.Process(pid)\n","    memory_use = py.memory_info()[0] / 2. ** 30\n","    memory_percent = psutil.virtual_memory().percent\n","    return np.round(memory_use, 2), np.round(memory_percent, 2)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-14T02:04:04.370492Z","iopub.status.busy":"2024-01-14T02:04:04.370150Z","iopub.status.idle":"2024-01-14T02:04:04.399647Z","shell.execute_reply":"2024-01-14T02:04:04.398537Z","shell.execute_reply.started":"2024-01-14T02:04:04.370461Z"},"trusted":true},"outputs":[],"source":["class ExperienceReplay:\n","    def __init__(self, max_replay_size):\n","        self.max_replay_size = max_replay_size\n","        self.experiences = deque(maxlen=max_replay_size)\n","\n","    def add_experience(self, obs, env, Q, eps):\n","        n_actions = env.action_space.n\n","        if eps >= random.random():\n","            action = random.randint(0, n_actions - 1)\n","        else:\n","            # We can also use numpy but this is more efficient\n","            tensor_state = tf.convert_to_tensor(obs)\n","            # Dimensions need to be expanded cause the model expects a batch/not a single element\n","            expanded_state = tf.expand_dims(tensor_state, axis=0)\n","            actions = Q(expanded_state, training=False)[0]\n","            action = tf.argmax(actions).numpy()\n","\n","        # Made to work with only one environment because I want to use StackedFrames\n","        # and it doesn't work with non vectorized environments\n","        next_obs, rew, done, info = env.step([action])\n","        rew = rew[0]\n","        done = done[0]\n","        info = info[0]\n","        next_obs = next_obs.squeeze()\n","\n","        self.experiences.append((obs, action, rew, next_obs, done))\n","\n","        return next_obs, rew, done, info\n","\n","    def sample_batch(self, batch_size):\n","        batch_idx = random.sample(range(0, len(self.experiences) - 1), batch_size)\n","\n","        batch_state = np.array([self.experiences[idx][0] for idx in batch_idx])\n","        batch_action = [self.experiences[idx][1] for idx in batch_idx]\n","        batch_rew = [self.experiences[idx][2] for idx in batch_idx]\n","        batch_next_state = np.array([self.experiences[idx][3] for idx in batch_idx])\n","        batch_done = tf.convert_to_tensor([float(self.experiences[idx][4]) for idx in batch_idx])\n","\n","        return batch_state, batch_action, batch_rew, batch_next_state, batch_done"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class CustomModel(tf.keras.Model):\n","    def __init__(self, n_stack, num_actions):\n","        super(CustomModel, self).__init__()\n","        self.conv1 = Conv2D(32, 8, strides=4, padding='valid', activation=\"relu\", input_shape=(84, 84, n_stack))\n","        self.conv2 = Conv2D(64, 4, strides=2, padding='valid', activation=\"relu\")\n","        self.conv3 = Conv2D(64, 3, strides=1, padding='valid', activation=\"relu\")\n","        self.flatten = Flatten()\n","        self.dense1 = Dense(512, activation=\"relu\")\n","        self.dense2 = Dense(num_actions, activation=\"linear\")\n","\n","    def call(self, inputs):\n","        inputs = tf.cast(inputs, dtype=tf.float32)\n","        x = self.conv1(inputs)\n","        x = self.conv2(x)\n","        x = self.conv3(x)\n","        x = self.flatten(x)\n","        x = self.dense1(x)\n","        x = self.dense2(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class LogData:\n","    def __init__(self, log_file_path, lr = 0.001, n_env=1, n_stack=4):\n","        self.log_file = log_file_path + '/data'\n","        self.log_dir = log_file_path\n","        self.data = []\n","        self.initial_size = 0\n","        self.last_print = 0\n","        self.n_stack = n_stack\n","        self.n_env = n_env\n","        self.lr = lr\n","        self.start_time = time.time()\n","        self.retrieve_highest_length_saved()\n","\n","    def add_episode_data(self, step, reward, losses, n_updates, eps_list):\n","        current_data = {}\n","        current_data['reward'] = int(reward * 10)\n","        current_data['eps'] = np.round(float(np.mean(eps_list)), 4)\n","        current_data['len'] = step - self.data[-1]['timesteps'] if len(self.data) > 1 else step\n","\n","        current_data['time'] = np.round(time.time() - self.start_time, 2)\n","        current_data['timesteps'] = step\n","\n","        cpu_stat = cpu_stats()\n","        current_data['cpu_memory'] = cpu_stat[0]\n","        current_data['cpu_percent'] = cpu_stat[1]\n","        \n","        current_data['learning_rate'] = self.lr\n","        current_data['loss_mean'] = np.round(float(np.mean(losses)), 2)\n","\n","        current_data['n_updates'] = (n_updates + self.data[-1]['n_updates']) if len(self.data) > 1 else n_updates\n","\n","        self.data.append(current_data)\n","\n","    def __len__(self):\n","        return len(self.data)\n","    \n","    def print_statistics(self):\n","        print(f\"{'='*28} Training Summary {'='*28}\")\n","        print(f\"Episode Length Mean: {self.ep_len_mean()}\")\n","        print(f\"Episode Reward Mean: {self.ep_rew_mean()}\")\n","        print(f\"Exploration Rate: {self.data[-1]['eps']}\")\n","        print(f\"Episodes: {self.episodes()}\")\n","        print(f\"Frame per second: {self.fps()}\")\n","        print(f\"Time Elapsed: {self.data[-1]['time']} seconds\")\n","        print(f\"Total Timesteps: {self.data[-1]['timesteps']}\")\n","        print(f\"CPU Memory Usage: {self.data[-1]['cpu_memory']} GB\")\n","        print(f\"CPU Usage: {self.data[-1]['cpu_percent']}%\")\n","        print(f\"Learning Rate: {self.data[-1]['learning_rate']}\")\n","        print(f\"Loss: {self.data[-1]['loss_mean']}\")\n","        print(f\"Num Updates: {self.data[-1]['n_updates']}\")\n","        print('='*74)\n","        self.last_print = len(self.data)\n","\n","        if self.data[-1]['cpu_percent'] > 80:\n","            print(\"WARNING: CPU usage is very high!\")\n","            print(\"Saving data to disk and freeing memory...\")\n","            self.save_data()\n","\n","    def ep_len_mean(self, window=-1):\n","        if window == -1:\n","            window = len(self.data) - self.last_print\n","        \n","        mean = np.mean([data['len'] for data in self.data[-window:]])\n","        return int(mean)\n","\n","    def ep_rew_mean(self, window=-1):\n","        if window < 0:\n","            window = len(self.data) - self.last_print\n","\n","        return int(np.mean([data['reward'] for data in self.data[-window:]]))\n","\n","    def episodes(self):\n","        return len(self.data) + self.initial_size\n","\n","    def fps(self, window=-1):\n","        if window == -1:\n","            window = len(self.data) - self.last_print\n","            \n","        frames = (self.data[-1]['timesteps'] - self.data[-window]['timesteps']) * self.n_env * self.n_stack\n","        seconds = self.data[-1]['time'] - self.data[-window]['time']\n","        return int(np.round(frames / seconds))\n","    \n","    def save_data(self):\n","        final_len = self.last_print + self.initial_size\n","        remain_len = len(self.data) - self.last_print\n","        self.last_print = 0 \n","\n","        if remain_len == 0:\n","            remain_len = 1\n","            final_len -= 1\n","            self.last_print += 1\n","\n","        if final_len == 0:\n","            return\n","        \n","        self.initial_size = final_len\n","        \n","        file_path = f\"{self.log_file}_{final_len}.json\"\n","\n","        if os.path.exists(file_path):\n","            return\n","\n","        with open(file_path, 'w') as file:\n","            json.dump(self.data[:-remain_len], file)\n","        \n","        self.data = self.data[-remain_len:]\n","        \n","        gc.collect()\n","\n","    @staticmethod\n","    def load_data(log_file_path):\n","        try:\n","            with open(log_file_path, 'r') as file:\n","                data = json.load(file)\n","        except FileNotFoundError:\n","            # Handle the case where the file does not exist\n","            data = []\n","        return data\n","    \n","    def retrieve_highest_length_saved(self):\n","        # Get a list of all files in the current directory\n","        files = [f for f in os.listdir(self.log_dir) if f.startswith(\"data_\") and f.endswith(\".json\")]\n","\n","        # Check if any files match the pattern\n","        if files:\n","            # Find the file with the highest length in its name\n","            highest_length_file = max(files, key=lambda x: int(x[len(\"data_\"):].split('.')[0]))\n","            filename = self.log_dir + \"/\" + highest_length_file\n","            self.initial_size = int(highest_length_file[len(\"data_\"):].split('.')[0])\n","\n","            self.data = self.load_data(filename)\n","            \n","            if len(self.data) > 1:\n","                os.remove(filename)\n","                self.last_print = len(self.data)\n","                self.initial_size -= len(self.data) \n","                self.save_data()\n","                \n","            gc.collect()\n","        else:\n","            self.initial_size = 0\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class DQNmodel:\n","    def __init__(self,\n","                model_path=\"./Q_model\",\n","                checkpoint_dir=\"./checkpoints\",\n","                env_path=\"ALE/MsPacman-v5\",\n","                dqn_type = \"vanilla\",\n","                n_stack=4,\n","                n_env=1,\n","                learning_rate = 0.00025,\n","                buffer_size = 100000,  # 1e6\n","                learning_starts: int = 1000,\n","                batch_size: int = 32,\n","                tau: float = 1.0, # soft update\n","                gamma: float = 0.99,\n","                update_target_Q_every_n_steps: int = 10, \n","                save_checkpoint_every_n_steps: int = 1000,\n","                log_every_n_episodes: int = 4,\n","                exploration_fraction: float = 0.1,\n","                exploration_initial_eps: float = 1.0,\n","                exploration_final_eps: float = 0.05,\n","                max_grad_norm: float = 10,\n","                total_lives: int = 3,\n","                avoid_finish_episode: bool = False\n","                 ):\n","        self.model_path = model_path\n","        self.checkpoint_dir = checkpoint_dir\n","        \n","        self.dqn_type = dqn_type\n","        if self.dqn_type in [\"vanilla\", \"double\", \"clipped_double\", \"dueling\", \"noisy\", \"prioritized\"]:\n","            self.model_path += \"_\" + self.dqn_type\n","            self.checkpoint_dir += \"_\" + self.dqn_type\n","        else:\n","            raise ValueError(\"Invalid DQN type: \" + self.dqn_type + \". Valid types are: vanilla, double, dueling, noisy\")\n","        \n","        # Create the directory if it doesn't exist\n","        if not os.path.exists(self.checkpoint_dir):\n","            os.makedirs(self.checkpoint_dir)\n","\n","        self.env_path = env_path\n","        self.n_env = n_env\n","        self.n_stack = n_stack\n","        self.learning_rate = learning_rate\n","        self.buffer_size = buffer_size\n","        self.learning_starts = learning_starts\n","        self.batch_size = batch_size\n","        self.tau = tau\n","        self.gamma = gamma\n","        self.update_target_Q_every_n_steps = update_target_Q_every_n_steps\n","        self.save_checkpoint_every_n_steps = save_checkpoint_every_n_steps\n","        self.log_every_n_episodes = log_every_n_episodes\n","        self.exploration_fraction = exploration_fraction\n","        self.exploration_initial_eps = exploration_initial_eps\n","        self.exploration_final_eps = exploration_final_eps\n","        self.max_grad_norm = max_grad_norm\n","        self.avoid_finish_episode = avoid_finish_episode\n","\n","        # Initialize environment\n","        self.env = VecFrameStack(make_atari_env(env_path), n_stack)\n","        self.num_actions = self.env.action_space.n\n","        self.total_lives = total_lives\n","\n","        # Initialize models\n","        self.Q_online = CustomModel(self.n_stack, self.num_actions)\n","        self.Q_target = CustomModel(self.n_stack, self.num_actions)\n","\n","        # Initialize training\n","        self.train_fn = self._get_train_fn()\n","        self.optimizer = tf.keras.optimizers.Adam(self.learning_rate, clipnorm=self.max_grad_norm)\n","        self.loss_function = tf.keras.losses.Huber()\n","        \n","        if self.dqn_type == \"clipped_double\":\n","            # Initialize training for target model\n","            self.train_fn_on_target = self._get_train_fn(target_model=True)\n","            self.optimizer_target = tf.keras.optimizers.Adam(self.learning_rate, clipnorm=self.max_grad_norm)\n","            self.loss_function_on_target = tf.keras.losses.Huber()\n","        \n","        # Initialize checkpoint\n","        if self.dqn_type == \"clipped_double\":\n","            self.checkpoint = tf.train.Checkpoint(step = tf.Variable(0), episode = tf.Variable(0), optimizer=self.optimizer, optimizer_target=self.optimizer_target, model=self.Q_target, model_online=self.Q_online)\n","        else:\n","            self.checkpoint = tf.train.Checkpoint(step = tf.Variable(0), episode = tf.Variable(0), optimizer=self.optimizer, model=self.Q_target, model_online=self.Q_online)\n","        \n","        self.checkpoint_manager = tf.train.CheckpointManager(self.checkpoint, self.checkpoint_dir, max_to_keep=1)\n","\n","        if self.checkpoint_manager.latest_checkpoint:\n","            self.checkpoint.restore(self.checkpoint_manager.latest_checkpoint)\n","            print(\"Restored from {}\".format(self.checkpoint_manager.latest_checkpoint))\n","        \n","        # Initialize buffer\n","        self.experience_replay = ExperienceReplay(self.buffer_size)\n","\n","        # Initialize logs\n","        self.log = LogData(self.checkpoint_dir, self.learning_rate, self.n_env, self.n_stack)\n","\n","        self.Q_online.set_weights(self.Q_target.get_weights())\n","    \n","    def _get_train_fn(self, target_model = False):\n","        @tf.function\n","        def train_function(batch_state, action_mask, target_q_values):\n","            # Optimize the policy\n","            with tf.GradientTape() as tape:\n","                # Get current Q-values estimates\n","                current_q_values = self.Q_online(batch_state, training=True)\n","\n","                current_q_values = tf.reduce_sum(tf.multiply(current_q_values, action_mask), axis=1)\n","\n","                loss = self.loss_function(target_q_values, current_q_values)\n","\n","            grads = tape.gradient(loss, self.Q_online.trainable_variables)\n","            \n","            # Clip gradient norm\n","            grads = [tf.clip_by_norm(g, self.max_grad_norm) for g in grads if g is not None]\n","            self.optimizer.apply_gradients(zip(grads, self.Q_online.trainable_variables))\n","\n","            return loss\n","\n","        # only used for clipped double\n","        @tf.function\n","        def train_function_on_target(batch_state, action_mask, target_q_values):\n","            # Optimize the policy\n","            with tf.GradientTape() as tape:\n","                # Get current Q-values estimates\n","                current_q_values = self.Q_target(batch_state, training=True)\n","\n","                current_q_values = tf.reduce_sum(tf.multiply(current_q_values, action_mask), axis=1)\n","\n","                loss = self.loss_function_on_target(target_q_values, current_q_values)\n","\n","            grads = tape.gradient(loss, self.Q_target.trainable_variables)\n","            \n","            # Clip gradient norm\n","            grads = [tf.clip_by_norm(g, self.max_grad_norm) for g in grads if g is not None]\n","            self.optimizer_target.apply_gradients(zip(grads, self.Q_target.trainable_variables))\n","\n","            return loss\n","        \n","        if target_model:\n","            return train_function_on_target\n","        \n","        return train_function\n","    \n","    def train(self, total_timesteps):\n","        # Initialize experience replay\n","        eps_it = LinearIterator(self.exploration_initial_eps, self.exploration_final_eps, total_timesteps * self.exploration_fraction)\n","\n","        # Initialize device\n","        if tf.config.list_physical_devices('GPU'):\n","            print(\"GPU is available\")\n","            device = '/GPU:0'\n","        else:\n","            print(\"GPU is not available, using CPU instead\")\n","            device = '/CPU:0'\n","\n","        with tf.device(device):\n","\n","            if int(self.checkpoint.step) == 0:\n","                print(\"\\nStarting training from scratch\")\n","            else:\n","                print(\"\\nResuming training from step\", int(self.checkpoint.step))\n","\n","            obs = self.env.reset().squeeze()\n","\n","            if int(self.checkpoint.step) > total_timesteps:\n","                print(\"\\nTraining already finished for this model and checkpoint\")\n","                print(\"Training steps:\", total_timesteps)\n","                print(\"Step count:\", int(self.checkpoint.step))\n","                return\n","            \n","            # Initial Warm Up\n","            print(\"Initial warm up:\", self.learning_starts, \"steps\")\n","            pbar = tqdm(range(self.learning_starts), desc=\"Warm up\", unit=\"step\")\n","            for count in pbar:\n","                eps = eps_it.value(count)\n","                next_obs, _, done, _ = self.experience_replay.add_experience(obs, self.env, self.Q_online, eps)\n","                obs = next_obs\n","            pbar.close()\n","    \n","            reward = 0\n","            losses = []\n","            n_updates = 0\n","            lives = self.total_lives\n","            eps_list = []\n","\n","            remaining_episodes_until_log = self.log_every_n_episodes - int(self.checkpoint.episode % self.log_every_n_episodes)\n","            pbar = tqdm(total = remaining_episodes_until_log, desc=\"Training\", unit=\"episode\")\n","            \n","            # Training\n","            while int(self.checkpoint.step) < total_timesteps:\n","                \n","                # New Experience\n","                eps = eps_it.value(int(self.checkpoint.step))\n","                next_obs, rew, done, _ = self.experience_replay.add_experience(obs, self.env, self.Q_online, eps)\n","                reward += rew\n","                eps_list.append(eps)\n","\n","                # Sample a batch from the experience replay\n","                batch_state, batch_action, batch_rew, batch_next_state, batch_done = self.experience_replay.sample_batch(self.batch_size)\n","\n","                # one hot encoded actions\n","                action_mask = tf.one_hot(batch_action, self.num_actions)\n","                \n","                if self.dqn_type == \"vanilla\":\n","                    # Compute the next Q-values using the target network\n","                    next_q_values_target = self.Q_target(batch_next_state, training=False)\n","                    \n","                    # Follow greedy policy: use the one with the highest value\n","                    next_q_values = tf.reduce_max(next_q_values_target, axis=1)\n","\n","                elif self.dqn_type == \"double\":\n","                    # Compute the next Q-values using the online network\n","                    next_q_values_online = self.Q_online(batch_next_state, training=False)\n","\n","                    # Follow greedy policy: use the action with the highest value\n","                    next_actions_online = tf.argmax(next_q_values_online, axis=1)\n","\n","                    # Compute the next Q-values using the target network\n","                    next_q_values_target = self.Q_target(batch_next_state, training=False)\n","\n","                    # Choose the Q-values of the actions chosen by the online network\n","                    next_q_values_target_best_actions = tf.reduce_sum(tf.one_hot(next_actions_online, self.num_actions) * next_q_values_target, axis=1, keepdims=True)\n","\n","                    next_q_values = next_q_values_target_best_actions\n","                elif self.dqn_type == \"clipped_double\":\n","                    # calculate both networks values\n","                    next_q_values_online = self.Q_online(batch_next_state, training=False)\n","                    next_q_values_target = self.Q_target(batch_next_state, training=False)\n","                    # Follow greedy policy: use the higher values\n","                    max_next_q_values_online = tf.reduce_max(next_q_values_online, axis=1)\n","                    max_next_q_values_target = tf.reduce_max(next_q_values_target, axis=1)\n","                    # calculate the minimum of the two networks values\n","                    next_q_values = tf.minimum(max_next_q_values_online, max_next_q_values_target)\n","\n","                \n","                if self.avoid_finish_episode:\n","                    # 1-step TD target\n","                    target_q_values = batch_rew + (1 - batch_done) * self.gamma * next_q_values - batch_done\n","                else:\n","                    # 1-step TD target\n","                    target_q_values = batch_rew + (1 - batch_done) * self.gamma * next_q_values\n","\n","                # Train the model\n","                loss = self.train_fn(batch_state, action_mask, target_q_values)\n","                if self.dqn_type == \"clipped_double\":\n","                    loss_target = self.train_fn_on_target(batch_state, action_mask, target_q_values)\n","                    loss = tf.reduce_mean([loss, loss_target])\n","                    \n","                losses.append(loss.numpy())\n","\n","                if done:\n","                    if lives > 0:\n","                        lives -= 1\n","                    else:\n","                        pbar.update(1)\n","                        self.checkpoint.episode.assign_add(1)\n","                        self.log.add_episode_data(int(self.checkpoint.step), reward, losses, n_updates, eps_list)\n","                        reward = 0\n","                        losses = []\n","                        n_updates = 0\n","                        lives = self.total_lives\n","                        eps_list = []\n","\n","                        # Logs depend on episodes\n","                        if int(self.checkpoint.episode) % self.log_every_n_episodes == 0:\n","                            self.log.print_statistics()\n","                            pbar.close()\n","                            pbar = tqdm(total=self.log_every_n_episodes, desc=\"Training\", unit=\"episode\")\n","                    \n","                    obs = self.env.reset().squeeze()\n","                else:\n","                    obs = next_obs\n","                \n","                self.checkpoint.step.assign_add(1)\n","\n","                # Update the target network\n","                if int(self.checkpoint.step) % self.update_target_Q_every_n_steps == 0 and not self.dqn_type == \"clipped_double\":\n","                    new_weights = [(1 - self.tau) * target + self.tau * online for target, online in zip(self.Q_target.get_weights(), self.Q_online.get_weights())]\n","                    self.Q_target.set_weights(new_weights)\n","                    gc.collect()\n","                    n_updates += 1\n","\n","                # Save checkpoint                   \n","                if int(self.checkpoint.step) % self.save_checkpoint_every_n_steps == 0:\n","                    # Save checkpoint\n","                    self.checkpoint_manager.save()\n","                    self.log.save_data()\n","                    gc.collect()\n","\n","            self.log.print_statistics()\n","            \n","            # Training finished, save the model      \n","            print(\"Saving model and logs\")          \n","            self.save_model()\n","            self.checkpoint_manager.save()\n","            self.log.save_data()\n","            print(\"Training finished\")\n","        \n","    def close(self):\n","        self.env.close()\n","\n","    def save_model(self):\n","        self.Q_target.save(self.model_path)\n","        if self.dqn_type == \"clipped_double\":\n","            self.Q_online.save(self.model_path + \"_online\")\n","\n","    @staticmethod\n","    def load_model(model_path):\n","        try:\n","            model = tf.keras.models.load_model(model_path)\n","            return model\n","        except:\n","            print(\"No model found\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["DQNmodel = DQNmodel(model_path=\"./model\",\n","                checkpoint_dir=\"./checkpoint\",\n","                env_path=\"ALE/MsPacman-v5\",\n","                n_stack=4,\n","                n_env=1,\n","                learning_rate = 0.00025,\n","                buffer_size = 100000,\n","                learning_starts = 10000,\n","                batch_size= 32,\n","                tau = 1.0, # no soft update\n","                gamma = 0.99,\n","                update_target_Q_every_n_steps = 10000, \n","                save_checkpoint_every_n_steps = 100000,\n","                log_every_n_episodes = 50,\n","                exploration_fraction = 0.4,\n","                exploration_initial_eps  = 1.0,\n","                exploration_final_eps  = 0.2,\n","                max_grad_norm = 10,\n","                dqn_type= \"clipped_double\",\n","                avoid_finish_episode=False\n","                )\n","DQNmodel.train(total_timesteps=1000000)\n","\n","DQNmodel.close()"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30636,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
