{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-01-12T10:32:44.096533Z","iopub.status.busy":"2024-01-12T10:32:44.096233Z","iopub.status.idle":"2024-01-12T10:33:27.518921Z","shell.execute_reply":"2024-01-12T10:33:27.517847Z","shell.execute_reply.started":"2024-01-12T10:32:44.096498Z"},"trusted":true},"outputs":[],"source":["#!pip install gymnasium[atari]\n","#!pip install gymnasium[accept-rom-license]"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-01-12T10:33:27.521256Z","iopub.status.busy":"2024-01-12T10:33:27.520921Z","iopub.status.idle":"2024-01-12T10:33:39.108724Z","shell.execute_reply":"2024-01-12T10:33:39.107623Z","shell.execute_reply.started":"2024-01-12T10:33:27.521220Z"},"trusted":true},"outputs":[],"source":["#!pip install \"gym[accept-rom-license, atari]\""]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-01-12 18:21:41.862348: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-01-12 18:21:41.862381: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-01-12 18:21:41.863738: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-01-12 18:21:41.870660: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-01-12 18:21:42.749124: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"]}],"source":["import gymnasium as gym\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras.layers import Conv2D, Flatten, Dense\n","from stable_baselines3.common.vec_env.vec_frame_stack import VecFrameStack\n","from stable_baselines3.common.env_util import make_atari_env\n","from keras.models import Sequential\n","import random\n","import os\n","import gc \n","import os, psutil \n","from tqdm import tqdm\n","import pandas as pd\n","from collections import deque\n","\n","gamma = 0.99                                    # Discount factor for past rewards\n","num_actions = 5                                 # 4 directions + no-op\n","\n","initial_sample_size = 1000                      # Number of steps to collect before starting training\n","batch_size = 32                                 # How many experiences to use for each training step\n","max_replay_size = 10000                         # The maximum size of the replay buffer\n","target_update_period = 100                      # The frequency with which the target network is updated\n","max_episode_rew_history = target_update_period  # How many past episodes to average over when calculating score\n","\n","experience_replay = []                          # A buffer used to store past experience_replay\n","episode_rew_history = []                        # A buffer used to store past episode rewards\n","episode_count = 0                               # A counter for the number of episodes\n","episode_rew = 0                                 # The reward accumulated over the current episode\n","training_episodes = 10000                       # The number of episodes to train for\n","avg_episodes_rew = 0                            # The reward total of the last 100 episodes\n","\n","#checkpoint_dir = '/kaggle/working/checkpoints'  # Where to save checkpoints\n","checkpoint_dir = './checkpoints'                 # Where to save checkpoints\n","checkpoint_every_n_update = 2                    # How often to checkpoint the model (in episodes)"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-01-12T10:32:44.082563Z","iopub.status.busy":"2024-01-12T10:32:44.082293Z","iopub.status.idle":"2024-01-12T10:32:44.094603Z","shell.execute_reply":"2024-01-12T10:32:44.092977Z","shell.execute_reply.started":"2024-01-12T10:32:44.082538Z"},"trusted":true},"outputs":[],"source":["class LinearIterator:\n","    def __init__(self, start, end, steps):\n","        self.start = start\n","        self.end = end\n","        self.steps = steps\n","    \n","    def value(self, step):\n","        return self.start + (self.end - self.start) * step / self.steps"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-01-12T10:33:39.110311Z","iopub.status.busy":"2024-01-12T10:33:39.110012Z","iopub.status.idle":"2024-01-12T10:33:39.115733Z","shell.execute_reply":"2024-01-12T10:33:39.114923Z","shell.execute_reply.started":"2024-01-12T10:33:39.110285Z"},"trusted":true},"outputs":[],"source":["def cpu_stats():\n","    pid = os.getpid()\n","    py = psutil.Process(pid)\n","    memory_use = py.memory_info()[0] / 2. ** 30\n","    memory_percent = psutil.virtual_memory().percent\n","    return np.round(memory_use, 2), np.round(memory_percent, 2)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-01-12T10:33:39.117875Z","iopub.status.busy":"2024-01-12T10:33:39.117618Z","iopub.status.idle":"2024-01-12T10:33:39.130424Z","shell.execute_reply":"2024-01-12T10:33:39.129761Z","shell.execute_reply.started":"2024-01-12T10:33:39.117852Z"},"trusted":true},"outputs":[],"source":["def get_train_fn():\n","    @tf.function\n","    def train_function(batch_state, action_mask, target, Q, loss_function, optimizer):\n","        with tf.GradientTape() as tape:\n","            q_pred = Q(batch_state)\n","            q_action = tf.reduce_sum(tf.multiply(q_pred, action_mask), axis=1)\n","            loss = loss_function(target, q_action)\n","\n","        grads = tape.gradient(loss, Q.trainable_variables)\n","        optimizer.apply_gradients(zip(grads, Q.trainable_variables))\n","        return loss\n","\n","    return train_function"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# Creates a simple convolutional NN to work with the images\n","def create_q_nn(num_actions):\n","    model = Sequential([\n","        Conv2D(32, 8, strides=4, activation=\"relu\", input_shape=(84, 84, 4)),\n","        Conv2D(64, 4, strides=2, activation=\"relu\"),\n","        Conv2D(64, 3, strides=1, activation=\"relu\"),\n","        Flatten(),\n","        Dense(512, activation=\"relu\"),\n","        Dense(num_actions, activation=\"linear\")\n","    ])\n","\n","    return model"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["class CheckpointManager:\n","    def __init__(self, checkpoint_dir, optimizer, model):\n","        self.checkpoint_dir = checkpoint_dir\n","        self.checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n","        self.checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model, last_episode_saved=tf.Variable(0, dtype=tf.int64))\n","            \n","        # Create the directory if it doesn't exist\n","        if not os.path.exists(checkpoint_dir):\n","            os.makedirs(checkpoint_dir)\n","\n","        # Restore the latest checkpoint if it exists\n","        if tf.train.latest_checkpoint(checkpoint_dir):\n","            self.checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n","            print(\"Restored from {}\".format(tf.train.latest_checkpoint(checkpoint_dir)))\n","            print(\"Last episode saved: {}\".format(self.checkpoint.last_episode_saved))\n","        else:\n","            print(\"Initializing from scratch.\")\n","            \n","\n","    def save_checkpoint(self, episode_count):\n","        print(\"Saving checkpoint...\")\n","        self.checkpoint.last_episode_saved = tf.Variable(episode_count, dtype=tf.int64)\n","        \n","        self.checkpoint.save(file_prefix=self.checkpoint_prefix)\n","        tf.keras.backend.clear_session()\n","        self.checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["import json\n","class ExperienceReplay:\n","    def __init__(self, max_replay_size, max_episode_rew_history, eps_it):\n","        self.max_replay_size = max_replay_size\n","        self.max_episode_rew_history = max_episode_rew_history\n","        self.eps_it = eps_it\n","        self.experiences = deque(maxlen=max_replay_size)\n","        self.episode_rew_history = deque(maxlen=max_episode_rew_history)\n","\n","        self.checkpoint_rew = []\n","        self.episode_rew = 0\n","\n","    def add_experience(self, episode_count, obs, env, Q):\n","        eps = self.eps_it.value(episode_count)\n","        if eps >= random.random():\n","            action = random.randint(0, 4)\n","        else:\n","            # We can also use numpy but this is more efficient\n","            tensor_state = tf.convert_to_tensor(obs)\n","            # Dimensions need to be expanded cause the model expects a batch/not a single element\n","            expanded_state = tf.expand_dims(tensor_state, axis=0)\n","            actions = Q(expanded_state, training=False)[0]\n","            action = tf.argmax(actions).numpy()\n","\n","        # Made to work with only one environment because I want to use StackedFrames\n","        # and it doesn't work with non vectorized environments\n","        next_obs, rew, done, info = env.step([action])\n","        rew = rew[0]\n","        done = done[0]\n","        info = info[0]\n","        next_obs = next_obs.squeeze()\n","\n","        self.episode_rew += rew\n","        \n","        if done:\n","            self.episode_rew_history.append(self.episode_rew)\n","            self.episode_rew = 0\n","\n","        self.experiences.append((obs, action, rew, next_obs, done))\n","\n","        return next_obs, rew, done, info\n","\n","    def sample_batch(self, batch_size):\n","        batch_idx = random.sample(range(0, len(self.experiences) - 1), batch_size)\n","\n","        batch_state = np.array([self.experiences[idx][0] for idx in batch_idx])\n","        batch_action = [self.experiences[idx][1] for idx in batch_idx]\n","        batch_rew = [self.experiences[idx][2] for idx in batch_idx]\n","        batch_next_state = np.array([self.experiences[idx][3] for idx in batch_idx])\n","        batch_done = tf.convert_to_tensor([float(self.experiences[idx][4]) for idx in batch_idx])\n","\n","        return batch_state, batch_action, batch_rew, batch_next_state, batch_done\n","\n","    def get_last_episode_rew(self):\n","        return self.episode_rew_history[-1]\n","\n","    def get_episode_rew_history(self):\n","        return self.episode_rew_history\n","    \n","    def get_avg_episode_rew(self):\n","        return np.mean(self.episode_rew_history)\n","    \n","    def update_checkpoint_rew(self):\n","        self.checkpoint_rew.append(self.get_avg_episode_rew())\n","\n","    def get_checkpoint_rew(self):\n","        return self.checkpoint_rew\n","    \n","    def save_checkpoint_rew(self, checkpoint_dir):\n","        # Save the checkpoint_rew list in a JSON file\n","        with open(checkpoint_dir + '/checkpoint_rew.json', 'w') as f:\n","            json.dump(list(self.checkpoint_rew), f)\n"]},{"cell_type":"code","execution_count":11,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n","[Powered by Stella]\n","2024-01-12 18:21:52.919241: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"]},{"name":"stdout","output_type":"stream","text":["Initializing from scratch.\n","GPU is not available, using CPU instead\n","Initial warm up: 1000 steps\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1000/1000 [00:07<00:00, 135.28it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Starting training\n"]},{"name":"stderr","output_type":"stream","text":["Episode: 100%|██████████| 10/10 [00:37<00:00,  3.77s/it, Last episode reward =2]\n"]},{"name":"stdout","output_type":"stream","text":["Running reward: 10.8 | Memory in use: 1.38 | Memory in use %: 52.8\n","\n","Saving checkpoint...\n"]},{"name":"stderr","output_type":"stream","text":["Episode: 100%|██████████| 10/10 [00:53<00:00,  5.31s/it, Last episode reward =21]\n"]},{"name":"stdout","output_type":"stream","text":["Running reward: 14.6 | Memory in use: 1.47 | Memory in use %: 53.8\n","\n"]},{"name":"stderr","output_type":"stream","text":["Episode: 100%|██████████| 10/10 [00:44<00:00,  4.44s/it, Last episode reward =13]\n"]},{"name":"stdout","output_type":"stream","text":["Running reward: 11.5 | Memory in use: 1.51 | Memory in use %: 56.6\n","\n","Saving checkpoint...\n"]},{"name":"stderr","output_type":"stream","text":["Episode: 100%|██████████| 10/10 [00:46<00:00,  4.67s/it, Last episode reward =11]\n"]},{"name":"stdout","output_type":"stream","text":["Running reward: 13.6 | Memory in use: 1.54 | Memory in use %: 55.4\n","\n"]},{"name":"stderr","output_type":"stream","text":["Episode: 100%|██████████| 10/10 [00:39<00:00,  3.93s/it, Last episode reward =10]\n"]},{"name":"stdout","output_type":"stream","text":["Running reward: 9.4 | Memory in use: 1.56 | Memory in use %: 55.8\n","\n","Saving checkpoint...\n"]},{"name":"stderr","output_type":"stream","text":["Episode: 100%|██████████| 10/10 [00:44<00:00,  4.48s/it, Last episode reward =7]\n"]},{"name":"stdout","output_type":"stream","text":["Running reward: 11.7 | Memory in use: 1.56 | Memory in use %: 55.9\n","\n"]},{"name":"stderr","output_type":"stream","text":["Episode:  60%|██████    | 6/10 [00:21<00:14,  3.60s/it, Last episode reward =7] \n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Repeat until episode is finished\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m# New Experience\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     next_obs, _, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43mexperience_replay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_experience\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisode_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQ\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Sample a batch from the experience replay\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     batch_state, batch_action, batch_rew, batch_next_state, batch_done \u001b[38;5;241m=\u001b[39m experience_replay\u001b[38;5;241m.\u001b[39msample_batch(batch_size)\n","Cell \u001b[0;32mIn[10], line 26\u001b[0m, in \u001b[0;36mExperienceReplay.add_experience\u001b[0;34m(self, episode_count, obs, env, Q)\u001b[0m\n\u001b[1;32m     22\u001b[0m     action \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39margmax(actions)\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Made to work with only one environment because I want to use StackedFrames\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# and it doesn't work with non vectorized environments\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m next_obs, rew, done, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m rew \u001b[38;5;241m=\u001b[39m rew[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     28\u001b[0m done \u001b[38;5;241m=\u001b[39m done[\u001b[38;5;241m0\u001b[39m]\n","File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/stable_baselines3/common/vec_env/vec_frame_stack.py:33\u001b[0m, in \u001b[0;36mVecFrameStack.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     32\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[np\u001b[38;5;241m.\u001b[39mndarray, Dict[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]], np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mndarray, List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]],]:\n\u001b[0;32m---> 33\u001b[0m     observations, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     observations, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstacked_obs\u001b[38;5;241m.\u001b[39mupdate(observations, dones, infos)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m observations, rewards, dones, infos\n","File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n","File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/gymnasium/core.py:461\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    459\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    460\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/gymnasium/core.py:555\u001b[0m, in \u001b[0;36mRewardWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[1;32m    553\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    554\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Modifies the :attr:`env` :meth:`step` reward using :meth:`self.reward`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 555\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m observation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward(reward), terminated, truncated, info\n","File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/gymnasium/core.py:522\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[1;32m    520\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    521\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 522\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, terminated, truncated, info\n","File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/stable_baselines3/common/atari_wrappers.py:112\u001b[0m, in \u001b[0;36mEpisodicLifeEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AtariStepReturn:\n\u001b[0;32m--> 112\u001b[0m     obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwas_real_done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# check current lives, make loss of life terminal,\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# then update lives to handle bonus lives\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/stable_baselines3/common/atari_wrappers.py:178\u001b[0m, in \u001b[0;36mMaxAndSkipEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    176\u001b[0m terminated \u001b[38;5;241m=\u001b[39m truncated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_skip):\n\u001b[0;32m--> 178\u001b[0m     obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m     done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_skip \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m:\n","File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/gymnasium/core.py:461\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    459\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    460\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/stable_baselines3/common/monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n","File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/gymnasium/wrappers/env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/shimmy/atari_env.py:294\u001b[0m, in \u001b[0;36mAtariEnv.step\u001b[0;34m(self, action_ind)\u001b[0m\n\u001b[1;32m    292\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(frameskip):\n\u001b[0;32m--> 294\u001b[0m     reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43male\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m is_terminal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mgame_over(with_truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    296\u001b[0m is_truncated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mgame_truncated()\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# make_atari_env creates an environment which reduces image sizes\n","# clips rewards in the range of -1, 0, 1 and replaces RGB with grayscale\n","# VecFrameStack does 4 steps and stackes them on each other so we \n","# can better train seeing how the Pacman moves and how the ghosts move\n","env = VecFrameStack(make_atari_env(\"ALE/MsPacman-v5\"), n_stack=4)\n","obs = env.reset().squeeze()\n","\n","# target fixed network\n","Q_target = create_q_nn(num_actions)\n","\n","# network we train\n","Q = create_q_nn(num_actions)\n","\n","train_fn = get_train_fn()\n","optimizer = tf.keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n","loss_function = keras.losses.Huber()\n","\n","eps_it = LinearIterator(1, 0.1, training_episodes)\n","\n","checkpoint_manager = CheckpointManager(checkpoint_dir, optimizer, Q)\n","\n","experience_replay = ExperienceReplay(max_replay_size, max_episode_rew_history, eps_it)\n","\n","if tf.config.list_physical_devices('GPU'):\n","    print(\"GPU is available\")\n","    device = '/GPU:0'\n","else:\n","    print(\"GPU is not available, using CPU instead\")\n","    device = '/CPU:0'\n","\n","with tf.device(device):\n","    # Maybe change to MSE\n","    tf.keras.backend.clear_session()\n","\n","    # Initial Warm Up\n","    print(\"Initial warm up:\", initial_sample_size, \"steps\")\n","    for count in tqdm(range(initial_sample_size)):\n","        next_obs, _, _, _ = experience_replay.add_experience(count, obs, env, Q)\n","        obs = next_obs\n","    \n","    # Training\n","    print(\"Starting training\")\n","    for count in range(training_episodes // target_update_period):\n","\n","        pbar = tqdm(range(target_update_period), desc=\"Episode\")\n","        for episode_count in pbar:\n","\n","            # Done if episode is finished\n","            done = False\n","\n","            # Repeat until episode is finished\n","            while not done:\n","                # New Experience\n","                next_obs, _, done, _ = experience_replay.add_experience(episode_count, obs, env, Q)\n","                \n","                # Sample a batch from the experience replay\n","                batch_state, batch_action, batch_rew, batch_next_state, batch_done = experience_replay.sample_batch(batch_size)\n","\n","                # one hot encoded actions\n","                action_mask = tf.one_hot(batch_action, num_actions)\n","                target_val = Q_target.predict(batch_next_state, verbose=0)\n","\n","                target = batch_rew + gamma * tf.reduce_max(target_val, axis=1)\n","\n","                # set last value to -1 if we have terminated. The goal is to avoid getting killed\n","                target = target * (1 - batch_done) - batch_done\n","\n","                # Actual training\n","                train_fn(batch_state, action_mask, target, Q, loss_function, optimizer)\n","\n","                # Update the current observation\n","                obs = next_obs\n","            \n","            \n","            obs = env.reset().squeeze()\n","            episode_rew = experience_replay.get_last_episode_rew()\n","            pbar.set_postfix({\"Last episode reward \": episode_rew})    \n","                \n","        gc.collect()\n","\n","        avg_episodes_rew = experience_replay.get_avg_episode_rew()\n","        cpu_stat = cpu_stats()\n","\n","        print(\"Running reward: {} | Memory in use: {} | Memory in use %: {}\".format(avg_episodes_rew, cpu_stat[0], cpu_stat[1]))\n","        print()\n","\n","        if count > 0 % checkpoint_every_n_update == 0:\n","            checkpoint_manager.save_checkpoint(episode_count)\n","            experience_replay.update_checkpoint_rew()\n","\n","        Q_target.set_weights(Q.get_weights())\n","\n","Q.save(\"./Q_model\")\n","env.close()\n","print(\"Training finished!\")\n","\n","experience_replay.save_checkpoint_rew(checkpoint_dir)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30627,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
